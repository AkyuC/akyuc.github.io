压缩主要分为以下 4 种技术
- 剪枝（Pruning）：移除模型中不必要或多余的组件，比如参数，以使模型更加高效；
- 知识蒸馏（Knowledge Distillation）：从一个复杂的模型（称为教师模型）向一个简化的模型（称为学生模型）转移知识来实现；
- 量化（Quantization）：将传统的表示方法中的浮点数转换为整数或其他离散形式，以减轻深度学习模型的存储和计算负担；
- 低秩分解（Low-Rank Factorization）：低秩分解旨在通过将给定的权重矩阵分解成两个或多个较小维度的矩阵，从而对其进行近似。

### 剪枝（Pruning）
剪枝主要分为：
- 非结构化剪枝：指移除个别参数，而不考虑整体网络结构。这种方法通过将低于阈值的参数置零的方式对个别权重或神经元进行处理。它会导致特定的参数被移除，模型出现不规则的稀疏结构。并且这种不规则性需要专门的压缩技术来存储和计算被剪枝的模型。此外，非结构化剪枝通常需要对 LLM 进行大量的再训练以恢复准确性，训练成本高。
- 结构化剪枝：根据预定义规则移除连接或分层结构，同时保持整体网络结构。这种方法一次性地针对整组权重，优势在于降低模型复杂性和内存使用，同时保持整体的 LLM 结构完整。

### 知识蒸馏（Knowledge Distillation）
知识蒸馏（KD），也被称为教师-学生神经网络学习算法，是一种有价值的机器学习技术，旨在提高模型性能和泛化能力。
核心思想是将教师模型的综合知识转化为更精简、更有效的表示。
根据是否将 LLM 的涌现能力（EA）提炼成小语言模型（SLM）来对蒸馏方法进行分类：标准知识蒸馏 和 基于 EA 的知识蒸馏。
> 当一个复杂系统由很多微小个体构成，这些微小个体凑到一起，相互作用，当数量足够多时，在宏观层面上展现出微观个体无法解释的特殊现象，就可以称之为“涌现现象”。
<img src="/img/llm-structure/KD-category.png" alt="KD-category" style="width: 80%; height: auto; display: block; margin: 0 auto;"  />

#### 标准知识蒸馏
Standard KD 旨在使学生模型学习 LLM 所拥有的常见知识，如输出分布和特征信息。这种方法类似于传统的 KD，但区别在于教师模型是 LLM。 比如：MINILLM 和 GKD。

我的理解是使用构建好的数据集（但是没有进行任务的拆分），输入到 LLM 当中，然后得到的输出，用来训练 SLM。
loss 函数是输出的 KL 散度，然后进行更新，有点类似回归。


#### 基于 EA 的知识蒸馏
基于 EA 的 KD 不仅仅迁移 LLM 的常识，还包括蒸馏他们的涌现能力。

与 BERT（330M）和 GPT-2（1.5B）等较小模型相比，GPT-3（175B）和 PaLM（540B）等 LLM 展示了独特的行为。 这些LLM在处理复杂的任务时表现出令人惊讶的能力，称为“涌现能力”。 
涌现能力包含三个方面，包括上下文学习 (ICL)、思维链 (CoT) 和指令遵循 (IF)。 如图三所示，它提供了基于 EA 的知识蒸馏概念的简明表示。

<img src="/img/llm-structure/EA-based-KD.png" alt="EA-based-KD" style="width: 80%; height: auto; display: block; margin: 0 auto;"  />

实际上就是构建不同类型的 prompt，输入到 LLM 中，构建出新的数据集，然后用来训练小模型
- ICL 采用结构化自然语言提示，其中包含任务描述以及可能的一些任务示例作为演示
- CoT 采用了不同的方法，它将中间推理步骤（可以导致最终输出）合并到提示中，而不是使用简单的输入输出对。
- IF 致力于仅基于阅读任务描述来增强语言模型执行新任务的能力，而不依赖于少数样本。通过使用一系列以指令表示的任务进行微调，语言模型展示了准确执行以前未见过的指令中描述的任务的能力。

### 量化（Quantization）

### 低秩分解（Low-Rank Factorization）

### Reference
- 大模型压缩首篇综述来啦~：https://zhuanlan.zhihu.com/p/652434165
- 大模型知识蒸馏概述：https://zhuanlan.zhihu.com/p/659943824